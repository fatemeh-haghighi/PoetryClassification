{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from file and split into test and train part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poams_data = pd.read_csv(\"Data/train_test.csv\")\n",
    "X_train, X_test = train_test_split(poams_data, test_size=0.2, random_state=12347, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Train class\n",
    "This class has some methods such as \"get_words\", \"calculate_probabilities\", \"get_max_result\", \"find_unknown_indexes\"\n",
    "and \"estimate\". at first \"get_words\" method makes a data structure of all the words that exist in lines of Hafez Poetry and number of repetitions of each word, and another data structure with the exact same form and data for Saadi.\n",
    "then \"calculate probabilities\" method calculates the probability of existence of each word in both data structure by number of their repetitions in each one.\n",
    "\"estimate\" method, estimates label of a poetry line by data trained. if a word does not exist in both data structure, we ignore it and do not consider the probability of that word. but if a word does not exist in just  a data structure we consider zero as its probability in this data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, data):\n",
    "        self.data = copy.deepcopy(data)\n",
    "        self.hafez_words = dict()\n",
    "        self.saadi_words = dict()\n",
    "        \n",
    "        self.saadi_word_number = 0\n",
    "        self.hafez_word_number = 0\n",
    "        \n",
    "        self.hafez_probabs = dict()\n",
    "        self.saadi_probabs = dict()\n",
    "        \n",
    "        self.saadi_self_probab = 0\n",
    "        self.hafez_self_probab = 0\n",
    "        \n",
    "        self.hafez_lines = 0\n",
    "        self.saadi_lines = 0\n",
    "\n",
    "    def get_words(self):\n",
    "        for i, j in self.data.iterrows():\n",
    "            if j.label == \"hafez\":\n",
    "                self.hafez_lines += 1\n",
    "            elif j.label == \"saadi\":\n",
    "                self.saadi_lines += 1\n",
    "                \n",
    "            for w in j.text.split():\n",
    "                if j.label == 'hafez':\n",
    "                    self.hafez_word_number += 1\n",
    "                    if w in self.hafez_words:\n",
    "                        self.hafez_words[w] += 1\n",
    "                    else:\n",
    "                        self.hafez_words[w] = 1\n",
    "                    \n",
    "                elif j.label == 'saadi':\n",
    "                    self.saadi_word_number += 1\n",
    "                    if w in self.saadi_words:\n",
    "                        self.saadi_words[w] += 1\n",
    "                    else:\n",
    "                        self.saadi_words[w] = 1\n",
    "        self.hafez_words[\"fatemeh\"] = 0\n",
    "        self.saadi_words[\"fatemeh\"] = 0\n",
    "                    \n",
    "    def calculate_probabilities(self):\n",
    "        self.hafez_self_probab = self.hafez_lines / (self.hafez_lines + self.saadi_lines)\n",
    "        self.saadi_self_probab = self.saadi_lines / (self.hafez_lines + self.saadi_lines)\n",
    "        for w in self.hafez_words:\n",
    "            self.hafez_probabs[w] = self.hafez_words[w] / self.hafez_word_number\n",
    "        for w in self.saadi_words:\n",
    "            self.saadi_probabs[w] = self.saadi_words[w] / self.saadi_word_number\n",
    "    \n",
    "    def get_max_result(self, hafez_estimation, saadi_estimation):\n",
    "        if hafez_estimation >= saadi_estimation:\n",
    "            return \"hafez\"\n",
    "        elif hafez_estimation < saadi_estimation:\n",
    "            return \"saadi\"\n",
    "    \n",
    "    def find_unknown_indexes(self, words):\n",
    "        remove_index = []\n",
    "        for i in range(len(words)):\n",
    "            is_hafez = 1\n",
    "            is_saadi = 1\n",
    "            if words[i] not in self.hafez_words:\n",
    "                is_hafez = 0\n",
    "            if words[i] not in self.saadi_words:\n",
    "                is_saadi = 0\n",
    "            if (is_hafez == 0) and (is_saadi == 0):\n",
    "                remove_index.append(i)\n",
    "        return remove_index\n",
    "        \n",
    "    def estimate(self, text):\n",
    "        words = text.split()\n",
    "        hafez_estimation = 1\n",
    "        saadi_estimation = 1\n",
    "        remove_index = self.find_unknown_indexes(words)\n",
    "        for index in range(len(words)):\n",
    "            if index in remove_index:\n",
    "                continue\n",
    "            if words[index] in self.hafez_words:\n",
    "                hafez_estimation *=  self.hafez_probabs[words[index]]\n",
    "            if words[index] not in self.hafez_words:\n",
    "                hafez_estimation *= self.hafez_probabs[\"fatemeh\"]\n",
    "            if words[index] in self.saadi_words:\n",
    "                saadi_estimation *= self.saadi_probabs[words[index]]\n",
    "            if words[index] not in self.saadi_words:\n",
    "                saadi_estimation *= self.saadi_probabs[\"fatemeh\"]\n",
    "        saadi_estimation *= self.saadi_self_probab\n",
    "        hafez_estimation *= self.hafez_self_probab\n",
    "        return self.get_max_result(hafez_estimation, saadi_estimation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Train(X_train)\n",
    "t.get_words()\n",
    "t.calculate_probabilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, t):\n",
    "    correct_detected_hafez = 0\n",
    "    correct_detected_saadi = 0\n",
    "    all_hafez = 0 \n",
    "    all_saadi = 0\n",
    "    detected_hafez = 0\n",
    "    detected_saadi = 0\n",
    "    total = 0\n",
    "    for i, j in X_test.iterrows():\n",
    "        total += 1\n",
    "        out = t.estimate(j.text)\n",
    "\n",
    "        if j.label == \"saadi\":\n",
    "            all_saadi += 1\n",
    "            if out == \"saadi\":\n",
    "                correct_detected_saadi += 1\n",
    "                detected_saadi += 1\n",
    "\n",
    "            if out == \"hafez\":\n",
    "                detected_hafez += 1\n",
    "\n",
    "        if j.label == \"hafez\":\n",
    "            all_hafez += 1\n",
    "            if out == \"hafez\":\n",
    "                correct_detected_hafez += 1\n",
    "                detected_hafez += 1\n",
    "            if out == \"saadi\":\n",
    "                detected_saadi += 1\n",
    "    return correct_detected_hafez, correct_detected_saadi, all_hafez, all_saadi, detected_hafez, detected_saadi, total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating recall for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saadi Recall is =  0.8194888178913738\n",
      "hafez Recall is =  0.7347670250896058\n"
     ]
    }
   ],
   "source": [
    "correct_detected_hafez,correct_detected_saadi,all_hafez,all_saadi,detected_hafez,detected_saadi,total = test(X_test, t)\n",
    "saadi_Recall = correct_detected_saadi / all_saadi\n",
    "hafez_Recall = correct_detected_hafez / all_hafez\n",
    "print(\"saadi Recall is = \", saadi_Recall)\n",
    "print(\"hafez Recall is = \", hafez_Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating precision for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saadi precision is =  0.8221153846153846\n",
      "hafez precision is =  0.7312722948870393\n"
     ]
    }
   ],
   "source": [
    "hafez_precision = correct_detected_hafez / detected_hafez\n",
    "saadi_precision = correct_detected_saadi / detected_saadi\n",
    "print(\"saadi precision is = \", saadi_precision)\n",
    "print(\"hafez precision is = \", hafez_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7855433221637147\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct_detected_hafez + correct_detected_saadi) / total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = pd.read_csv(\"Data/evaluate.csv\")\n",
    "labels = []\n",
    "for i, j in eval_data.iterrows():\n",
    "    labels.append(t.estimate(j.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data[\"estimated_label\"] = labels\n",
    "export_csv = eval_data.to_csv (r'Data/evaluate.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing laplace smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Laplas(Train):\n",
    "     def calculate_probabilities(self):\n",
    "        self.hafez_self_probab = self.hafez_lines / (self.hafez_lines + self.saadi_lines)\n",
    "        self.saadi_self_probab = self.saadi_lines / (self.hafez_lines + self.saadi_lines)\n",
    "        \n",
    "        a = 0.4\n",
    "        \n",
    "        for w in self.hafez_words:\n",
    "            self.hafez_probabs[w] = (self.hafez_words[w] + a) / (self.hafez_word_number + (a * len(self.hafez_words)))\n",
    "        for w in self.saadi_words:\n",
    "            self.saadi_probabs[w] = (self.saadi_words[w] + a) / (self.saadi_word_number + (a * len(self.saadi_words)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Laplas(X_train)\n",
    "l.get_words()\n",
    "l.calculate_probabilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating recall for Train based on Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saadi Recall is =  0.8514376996805112\n",
      "hafez Recall is =  0.7479091995221028\n"
     ]
    }
   ],
   "source": [
    "correct_detected_hafez,correct_detected_saadi,all_hafez,all_saadi,detected_hafez,detected_saadi,total = test(X_test, l)\n",
    "saadi_Recall = correct_detected_saadi / all_saadi\n",
    "hafez_Recall = correct_detected_hafez / all_hafez\n",
    "print(\"saadi Recall is = \", saadi_Recall)\n",
    "print(\"hafez Recall is = \", hafez_Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating precision for Train based on Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saadi precision is =  0.8347689898198903\n",
      "hafez precision is =  0.770935960591133\n"
     ]
    }
   ],
   "source": [
    "hafez_precision = correct_detected_hafez / detected_hafez\n",
    "saadi_precision = correct_detected_saadi / detected_saadi\n",
    "print(\"saadi precision is = \", saadi_precision)\n",
    "print(\"hafez precision is = \", hafez_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating accuracy for Train based on Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8099569171852561\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct_detected_hafez + correct_detected_saadi) / total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/evaluate.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(axis = 1 ,columns = 'text' , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
